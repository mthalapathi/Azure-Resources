# Azure Resources

## Azure Portal
- [Azure Portal](https://portal.azure.com/#home)

## Udemy Practice Labs
- [Udemy Practice Labs](https://dxclearning.edcast.com/insights/limited-access-build-in-demand-tech-skills-with-udemy-pro-f)

## Azure Copy
- [Azure Copy Documentation](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10?tabs=dnf)
- [YouTube - Azure Copy](https://www.youtube.com/watch?v=1JdnKzuta80)

---

# Hierarchical Namespace (HNS) in Azure Data Lake Storage Gen2

The **Hierarchical Namespace (HNS)** in Azure Data Lake Storage Gen2 transforms your Blob Storage to behave like a typical file system, with folders and files.

## Without HNS (Normal Blob Storage):
- **Structure:** Everything is stored as blobs in a "flat" structure.
- **Example:** A blob might have the name `folder1/file1.txt`. But, Azure treats it as a single file, with no real concept of a folder.
- **Operations:** No folder management, so deleting a "folder" means deleting every blob manually with names starting with the folder prefix.

## With HNS:
- **Structure:** Files and folders are treated as distinct objects, just like on your computer.
- **Example:** Uploading a `folder1/file1.txt` will create:
  - A folder called `folder1`
  - A file called `file1.txt` inside that folder
- **Operations:** You can independently rename, move, or delete entire folders and files.

## Key Differences

| Feature                      | Flat Namespace (Normal Blob Storage)              | Hierarchical Namespace                       |
|------------------------------|----------------------------------------------------|---------------------------------------------|
| **Folder Structure**          | Flat list of blobs, no real folders.              | Files are arranged in real folders.        |
| **Folder Deletion**           | Must delete all blobs with names starting with the folder name. | Delete the folder directly, and all files inside are deleted automatically. |
| **Renaming Folders**          | Not possible. Must copy blobs with new names and delete the old ones. | Directly rename folders.                   |
| **List Folder Contents**      | Filter blobs starting with the folder prefix.      | List files and subfolders under the folder directly. |

## Why Use HNS?

- **Better Organization:** Navigate through files and subfolders just like a traditional file system.
- **Efficient Operations:** Perform operations like renaming, moving, or deleting entire folders, rather than individual files.
- **Required for Data Lake Features:** Essential for advanced analytics and processing with tools like Hadoop or Spark.

---

## Use Case Comparison: Flat Namespace vs. Hierarchical Namespace

| **Action**                   | **Flat Namespace**                                   | **Hierarchical Namespace**                           |
|------------------------------|------------------------------------------------------|------------------------------------------------------|
| **Delete a folder**           | Delete all blobs starting with the folder prefix.   | Delete the folder directly, and everything inside is removed. |
| **Rename a folder**           | Not possible. Must copy and delete blobs individually. | Rename the folder directly.                        |
| **List contents of a folder** | Filter blobs by prefix.                             | List files and subfolders directly inside the folder. |

## Conclusion

- If you need to manage data in a folder-like structure, **HNS** is the better choice.
- It simplifies operations like renaming, deleting, and moving large datasets.
- **HNS** is also necessary for advanced analytics tools such as Spark, Hadoop, or Azure Data Lake.

## Storage Event Trigger and How To Automate Your ADF Pipeline Run
- [Youtube Portal] (https://youtu.be/c7ZuVecWeCs?si=FWyXX8cxLFNRDT8Q)

## ADF Portal
- [ADF Portal] - (https://www.youtube.com/playlist?list=PLm7Nm9btqfe3QNbrLY-Vyu8-wh3EanG6t)

\
# Azure Data Factory (ADF) Features: Parameters, Variables, Functions, and System Variables

## Overview

Below is a detailed representation of key features in Azure Data Factory (ADF), including definitions, use cases, and examples.

| **Feature**      | **Definition** | **Use Case** | **Example** |
|------------------|----------------|--------------|-------------|
| **Parameter**    | A read-only, externally passed value into a pipeline or dataset, specified at runtime. | Dynamically pass values like file paths, connection strings, or control values for reuse. | Pipeline Parameter: `SourceFileName = employee.csv`<br>Used in a dataset: `@pipeline().parameters.SourceFileName` |
| **Variable**     | A mutable storage of values within a pipeline for conditional logic or iterative operations. | Store intermediate values during pipeline execution. | Set Variable: `@add(variables('currentValue'), 1)` |
| **Function**     | Built-in expressions used to manipulate or evaluate data. | Perform transformations, calculations, or format changes. | Expression: `@concat('prefix_', pipeline().parameters.fileName)` |
| **System Variable** | Predefined variables in ADF to access metadata about pipeline runs, triggers, and execution. | Retrieve metadata like pipeline names, run IDs, or trigger start times for logging or auditing. | Pipeline Name: `@pipeline().Pipeline`<br>Trigger Time: `@trigger().startTime` |

### Detailed Examples and Use Cases

| **Feature**      | **Use Case** | **Example** |
|------------------|--------------|-------------|
| **Parameter**    | Dynamically define the file path for a source dataset in a pipeline. | Pipeline Parameter: `FilePath = folder1/2024/12/employee.csv`<br>Used in a dataset: `@pipeline().parameters.FilePath` |
| **Variable**     | Increment a counter for iteration in a `ForEach` loop. | Set Variable Activity: `counter = @add(variables('counter'), 1)` |
| **Function**     | Format the current UTC time to include it in a file name. | Expression: `@concat('output_', formatDateTime(utcnow(), 'yyyyMMddHHmmss'), '.csv')` |
| **System Variable** | Log the pipeline run ID for audit or tracking purposes. | Pipeline Run ID: `@pipeline().RunId` |
| **Parameter**    | Specify connection details for datasets to be dynamically assigned at runtime. | Dataset Parameter: `DatabaseName = SalesDB`<br>Reference in connection: `@dataset().DatabaseName` |
| **Variable**     | Store the status of a specific activity execution for conditional execution in the pipeline. | Set Variable Activity: `Status = @activity('Copy Data').output.Status` |
| **Function**     | Evaluate whether a file name contains a specific substring for filtering files in a `ForEach` loop. | Expression: `@contains(item().name, '2024')` |
| **System Variable** | Use the trigger start time to filter data based on the pipeline execution window. | Trigger Start Time: `@trigger().startTime` |

### Conclusion

This table provides a concise overview of how parameters, variables, functions, and system variables are used in Azure Data Factory (ADF). By using these features effectively, you can build dynamic, efficient, and flexible data pipelines.

### The article on Medium explains how to parameterize datasets in Azure Data Factory (ADF). It covers steps to make datasets dynamic by using parameters, 
making it possible to pass values at runtime for items like file paths or database names. 
The guide also discusses configuring parameters in linked services, datasets, and pipelines for flexibility in managing data flows.

### For detailed instructions, visit the full article:
- https://dataengineerazure.medium.com/how-to-parameterize-dataset-in-azure-data-factory-parameterization-in-azure-data-factory-4cae57d886aa
- https://dataengineerazure.medium.com/copying-multiple-files-in-azure-data-factory-how-to-copy-multiple-files-using-single-copy-b3c1d15ddbd9

### Virtual Machines
- [Azure Portal] - (https://learn.microsoft.com/en-us/azure/virtual-machines/windows/quick-create-portal)

### AZ-204: Course for Developing Solutions for Microsoft Azure
- [Azure Portal]  - (https://dxc.udemy.com/course/azure-certification-1/learn/lecture/31955592#labs)

